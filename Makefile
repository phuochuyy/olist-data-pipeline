.PHONY: help setup start stop restart logs status clean test dbt-run dbt-test spark-shell spark-bash pyspark spark-test psql backup urls

# Default target
help:
	@echo "ğŸš€ Olist Data Engineering Pipeline Commands"
	@echo ""
	@echo "Setup Commands:"
	@echo "  setup         - Initial setup of the pipeline"
	@echo "  start         - Start all services"
	@echo "  stop          - Stop all services"
	@echo "  restart       - Restart all services"
	@echo ""
	@echo "Monitoring Commands:"
	@echo "  logs          - Show logs from all services"
	@echo "  status        - Show status of all services"
	@echo ""
	@echo "Development Commands:"
	@echo "  dbt-run       - Run dbt transformations"
	@echo "  dbt-test      - Run dbt tests"
	@echo "  spark-shell   - Open Spark shell"
	@echo "  psql          - Connect to PostgreSQL"
	@echo ""
	@echo "Maintenance Commands:"
	@echo "  clean         - Clean up containers and volumes"
	@echo "  test          - Run data quality tests"
	@echo "  backup        - Backup database"

# Setup pipeline
setup:
	@echo "ğŸ”§ Setting up Olist Data Pipeline..."
	chmod +x setup.sh
	./setup.sh

# Start services
start:
	@echo "ğŸš€ Starting services..."
	docker compose up -d
	@echo "âœ… Services started!"

# Stop services
stop:
	@echo "ğŸ›‘ Stopping services..."
	docker compose down
	@echo "âœ… Services stopped!"

# Restart services
restart: stop start
	@echo "ğŸ”„ Services restarted!"

# Show logs
logs:
	@echo "ï¿½ Showing logs..."
	docker compose logs -f

# Show service status
status:
	@echo "ï¿½ Service status:"
	docker compose ps

# Run dbt transformations
dbt-run:
	@echo "ğŸ”„ Running dbt transformations..."
	docker compose exec airflow-webserver bash -c "cd /opt/airflow/dbt_project && dbt run"

# Run dbt tests
dbt-test:
	@echo "ğŸ§ª Running dbt tests..."
	docker compose exec airflow-webserver bash -c "cd /opt/airflow/dbt_project && dbt test"

# Open Spark shell (with instructions due to ivy config issue)
spark-shell:
	@echo "ï¿½ Spark Shell Setup:"
	@echo ""
	@echo "Due to Bitnami Spark ivy configuration, use manual setup:"
	@echo "1. Run: make spark-bash"
	@echo "2. Inside container run: python3"
	@echo "3. Setup PySpark:"
	@echo "   from pyspark.sql import SparkSession"
	@echo "   spark = SparkSession.builder.appName('Interactive').master('local[*]').getOrCreate()"
	@echo "   spark.range(5).show()  # Test"
	@echo ""
	@make spark-bash

# Access Spark container bash
spark-bash:
	@echo "ğŸ’» Opening Spark container bash..."
	@echo "ğŸ’¡ Inside container you can run PySpark commands"
	docker compose exec spark-master bash

# PySpark shell (alias to spark-shell)
pyspark: spark-shell

# Test Spark installation
spark-test:
	@echo "ğŸ§ª Testing Spark installation..."
	docker compose exec spark-master python3 -c "import pyspark; print('âœ… PySpark version:', pyspark.__version__)"

# Connect to PostgreSQL
psql:
	@echo "ğŸ—„ï¸ Connecting to PostgreSQL..."
	docker compose exec postgres psql -U postgres -d olist_dw

# Run data quality tests
test:
	@echo "ğŸ§ª Running data quality tests..."
	docker compose exec airflow-webserver python3 /opt/airflow/great_expectations/data_quality.py

# Backup database
backup:
	@echo "ğŸ’¾ Creating database backup..."
	docker compose exec postgres pg_dump -U postgres olist_dw > backup_$(shell date +%Y%m%d_%H%M%S).sql
	@echo "âœ… Backup completed!"

# Clean up
clean:
	@echo "ğŸ§¹ Cleaning up..."
	docker compose down -v
	docker system prune -f
	@echo "âœ… Cleanup completed!"

# Show service URLs
urls:
	@echo "ğŸŒ Service URLs:"
	@echo "â€¢ Airflow:  http://localhost:8080 (admin/admin)"
	@echo "â€¢ Grafana:  http://localhost:3000 (admin/admin)"
	@echo "â€¢ Jupyter:  http://localhost:8888"
	@echo "â€¢ Spark UI: http://localhost:8081"