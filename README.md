# ğŸš€ Olist Data Engineering Pipeline


Pipeline nÃ y Ä‘Æ°á»£c thiáº¿t káº¿ theo mÃ´ hÃ¬nh **Medallion Architecture** (Bronze-Silver-Gold) vá»›i cÃ¡c layers:

- **Bronze Layer (Raw)**: Dá»¯ liá»‡u thÃ´ tá»« CSV files
- **Silver Layer (Staging)**: Dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c lÃ m sáº¡ch vÃ  chuáº©n hÃ³a  
- **Gold Layer (Marts)**: Dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c tá»•ng há»£p vÃ  sáºµn sÃ ng cho analytics

### CÃ´ng nghá»‡ sá»­ dá»¥ng

| ThÃ nh pháº§n | CÃ´ng nghá»‡ | Má»¥c Ä‘Ã­ch |
|------------|-----------|----------|
| **Orchestration** | Apache Airflow | Äiá»u phá»‘i vÃ  láº­p lá»‹ch pipeline |
| **Data Processing** | Apache Spark | Xá»­ lÃ½ dá»¯ liá»‡u quy mÃ´ lá»›n |
| **Data Warehouse** | PostgreSQL | LÆ°u trá»¯ dá»¯ liá»‡u |
| **Data Transformation** | dbt | Transformation vÃ  modeling |
| **Data Quality** | Great Expectations | Kiá»ƒm tra cháº¥t lÆ°á»£ng dá»¯ liá»‡u |
| **Monitoring** | Grafana | GiÃ¡m sÃ¡t vÃ  visualization |
| **Caching** | Redis | Cache cho hiá»‡u suáº¥t |
| **Containerization** | Docker | Container hÃ³a services |
| **Exploration** | Jupyter Lab | Data exploration vÃ  analysis |

## ğŸ“Š Dataset

Dá»¯ liá»‡u Olist bao gá»“m 9 báº£ng chÃ­nh:

1. **olist_customers_dataset.csv** - ThÃ´ng tin khÃ¡ch hÃ ng
2. **olist_orders_dataset.csv** - ThÃ´ng tin Ä‘Æ¡n hÃ ng
3. **olist_order_items_dataset.csv** - Chi tiáº¿t sáº£n pháº©m trong Ä‘Æ¡n hÃ ng
4. **olist_order_payments_dataset.csv** - ThÃ´ng tin thanh toÃ¡n
5. **olist_order_reviews_dataset.csv** - ÄÃ¡nh giÃ¡ cá»§a khÃ¡ch hÃ ng
6. **olist_products_dataset.csv** - Catalog sáº£n pháº©m
7. **olist_sellers_dataset.csv** - ThÃ´ng tin ngÆ°á»i bÃ¡n
8. **olist_geolocation_dataset.csv** - Dá»¯ liá»‡u Ä‘á»‹a lÃ½
9. **product_category_name_translation.csv** - Dá»‹ch tÃªn danh má»¥c

## ğŸš€ HÆ°á»›ng dáº«n cÃ i Ä‘áº·t

### CÃ i Ä‘áº·t nhanh

```bash

cd ...

# Cháº¡y script cÃ i Ä‘áº·t
make setup

# Hoáº·c cháº¡y thá»§ cÃ´ng
chmod +x data_pipeline/setup.sh
./data_pipeline/setup.sh
```

### CÃ i Ä‘áº·t tá»«ng bÆ°á»›c

1. **Khá»Ÿi Ä‘á»™ng services**:
```bash
make start
```

2. **Kiá»ƒm tra tráº¡ng thÃ¡i**:
```bash
make status
```

3. **Xem logs**:
```bash
make logs
```

## ğŸ¯ Sá»­ dá»¥ng

### 1. Truy cáº­p cÃ¡c giao diá»‡n

| Service | URL | Username | Password |
|---------|-----|----------|----------|
| Airflow | http://localhost:8080 | admin | admin |
| Spark UI | http://localhost:8081 | - | - |
| Grafana | http://localhost:3000 | admin | admin |
| Jupyter Lab | http://localhost:8888 | - | - |

### 2. Cháº¡y Pipeline

1. Truy cáº­p Airflow UI (http://localhost:8080)
2. ÄÄƒng nháº­p vá»›i admin/admin
3. TÃ¬m DAG `olist_data_pipeline`
4. Enable DAG vÃ  trigger cháº¡y

### 3. GiÃ¡m sÃ¡t Pipeline

- **Airflow**: Theo dÃµi task execution vÃ  logs
- **Spark UI**: GiÃ¡m sÃ¡t Spark jobs vÃ  performance
- **Grafana**: Dashboards cho business metrics
- **Jupyter**: Data exploration vÃ  analysis



## ğŸ”„ Data Flow

```
CSV Files â†’ Raw Layer (Bronze) â†’ Staging Layer (Silver) â†’ Marts Layer (Gold) â†’ Analytics
    â†“              â†“                    â†“                   â†“              â†“
 Ingestion    Data Quality      Transformation        Aggregation    Visualization
```

### Luá»“ng xá»­ lÃ½ chi tiáº¿t:

1. **Data Ingestion**: Spark Ä‘á»c CSV files vÃ  load vÃ o raw schema
2. **Data Quality**: Great Expectations kiá»ƒm tra cháº¥t lÆ°á»£ng dá»¯ liá»‡u
3. **Data Transformation**: dbt transform data tá»« raw â†’ staging â†’ marts
4. **Data Validation**: Kiá»ƒm tra business rules vÃ  data consistency
5. **Analytics**: Táº¡o dashboards vÃ  reports trong Grafana



## ğŸ”§ Cáº¥u hÃ¬nh

### Environment Variables

Táº¡o file `.env` trong thÆ° má»¥c `data_pipeline`:

```env
# Database
POSTGRES_DB=olist_dw
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres

# Airflow
AIRFLOW__CORE__FERNET_KEY=your-fernet-key
AIRFLOW__WEBSERVER__SECRET_KEY=your-secret-key

# Spark
SPARK_WORKER_MEMORY=2G
SPARK_WORKER_CORES=2
```

### Scaling

Äá»ƒ scale pipeline cho production:

1. **TÄƒng Spark workers**:
```yaml
spark-worker:
  deploy:
    replicas: 3
```

2. **Cáº¥u hÃ¬nh Airflow Executor**:
```yaml
AIRFLOW__CORE__EXECUTOR: CeleryExecutor
```

3. **ThÃªm monitoring**:
- Prometheus metrics
- ELK stack for logging
- Alerting vá»›i PagerDuty/Slack


## ğŸ”§ Development & Deployment

### Local Development

1. **Clone repository**:
```bash
git clone https://github.com/your-username/olist-data-pipeline.git
cd olist-data-pipeline
```

2. **Copy environment files**:
```bash
cp .env.example .env
cp dbt_project/profiles.yml.example dbt_project/profiles.yml
```

3. **Run setup**:
```bash
make setup
make up
```



## ğŸ“„ License

MIT License - xem file [LICENSE](LICENSE) Ä‘á»ƒ biáº¿t thÃªm chi tiáº¿t.

