# ğŸš€ Olist Data Engineering Pipeline

Má»™t pipeline data engineering hiá»‡n Ä‘áº¡i vÃ  toÃ n diá»‡n cho viá»‡c xá»­ lÃ½ vÃ  phÃ¢n tÃ­ch dá»¯ liá»‡u thÆ°Æ¡ng máº¡i Ä‘iá»‡n tá»­ Olist (Brazil).

[![Docker](https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white)](https://www.docker.com/)
[![Apache Airflow](https://img.shields.io/badge/Apache%20Airflow-017CEE?style=for-the-badge&logo=apache-airflow&logoColor=white)](https://airflow.apache.org/)
[![Apache Spark](https://img.shields.io/badge/Apache%20Spark-E25A1C?style=for-the-badge&logo=apache-spark&logoColor=white)](https://spark.apache.org/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-4169E1?style=for-the-badge&logo=postgresql&logoColor=white)](https://www.postgresql.org/)
[![dbt](https://img.shields.io/badge/dbt-FF694B?style=for-the-badge&logo=dbt&logoColor=white)](https://www.getdbt.com/)

> **Note**: ÄÃ¢y lÃ  má»™t dá»± Ã¡n demo Ä‘á»ƒ minh há»a kiáº¿n trÃºc data engineering pipeline hiá»‡n Ä‘áº¡i. Code Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a cho mÃ´i trÆ°á»ng há»c táº­p vÃ  phÃ¡t triá»ƒn.

## ğŸ—ï¸ Kiáº¿n trÃºc

Pipeline nÃ y Ä‘Æ°á»£c thiáº¿t káº¿ theo mÃ´ hÃ¬nh **Medallion Architecture** (Bronze-Silver-Gold) vá»›i cÃ¡c layers:

- **Bronze Layer (Raw)**: Dá»¯ liá»‡u thÃ´ tá»« CSV files
- **Silver Layer (Staging)**: Dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c lÃ m sáº¡ch vÃ  chuáº©n hÃ³a  
- **Gold Layer (Marts)**: Dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c tá»•ng há»£p vÃ  sáºµn sÃ ng cho analytics

### CÃ´ng nghá»‡ sá»­ dá»¥ng

| ThÃ nh pháº§n | CÃ´ng nghá»‡ | Má»¥c Ä‘Ã­ch |
|------------|-----------|----------|
| **Orchestration** | Apache Airflow | Äiá»u phá»‘i vÃ  láº­p lá»‹ch pipeline |
| **Data Processing** | Apache Spark | Xá»­ lÃ½ dá»¯ liá»‡u quy mÃ´ lá»›n |
| **Data Warehouse** | PostgreSQL | LÆ°u trá»¯ dá»¯ liá»‡u |
| **Data Transformation** | dbt | Transformation vÃ  modeling |
| **Data Quality** | Great Expectations | Kiá»ƒm tra cháº¥t lÆ°á»£ng dá»¯ liá»‡u |
| **Monitoring** | Grafana | GiÃ¡m sÃ¡t vÃ  visualization |
| **Caching** | Redis | Cache cho hiá»‡u suáº¥t |
| **Containerization** | Docker | Container hÃ³a services |
| **Exploration** | Jupyter Lab | Data exploration vÃ  analysis |

## ğŸ“Š Dataset

Dá»¯ liá»‡u Olist bao gá»“m 9 báº£ng chÃ­nh:

1. **olist_customers_dataset.csv** - ThÃ´ng tin khÃ¡ch hÃ ng
2. **olist_orders_dataset.csv** - ThÃ´ng tin Ä‘Æ¡n hÃ ng
3. **olist_order_items_dataset.csv** - Chi tiáº¿t sáº£n pháº©m trong Ä‘Æ¡n hÃ ng
4. **olist_order_payments_dataset.csv** - ThÃ´ng tin thanh toÃ¡n
5. **olist_order_reviews_dataset.csv** - ÄÃ¡nh giÃ¡ cá»§a khÃ¡ch hÃ ng
6. **olist_products_dataset.csv** - Catalog sáº£n pháº©m
7. **olist_sellers_dataset.csv** - ThÃ´ng tin ngÆ°á»i bÃ¡n
8. **olist_geolocation_dataset.csv** - Dá»¯ liá»‡u Ä‘á»‹a lÃ½
9. **product_category_name_translation.csv** - Dá»‹ch tÃªn danh má»¥c

## ğŸš€ HÆ°á»›ng dáº«n cÃ i Ä‘áº·t

### YÃªu cáº§u há»‡ thá»‘ng

- Docker & Docker Compose
- 8GB RAM trá»Ÿ lÃªn
- 10GB dung lÆ°á»£ng trá»‘ng

### CÃ i Ä‘áº·t nhanh

```bash
# Clone repository vÃ  di chuyá»ƒn vÃ o thÆ° má»¥c dá»± Ã¡n
cd /home/phuochuy/Projects/pipeline05

# Cháº¡y script cÃ i Ä‘áº·t
make setup

# Hoáº·c cháº¡y thá»§ cÃ´ng
chmod +x data_pipeline/setup.sh
./data_pipeline/setup.sh
```

### CÃ i Ä‘áº·t tá»«ng bÆ°á»›c

1. **Khá»Ÿi Ä‘á»™ng services**:
```bash
make start
```

2. **Kiá»ƒm tra tráº¡ng thÃ¡i**:
```bash
make status
```

3. **Xem logs**:
```bash
make logs
```

## ğŸ¯ Sá»­ dá»¥ng

### 1. Truy cáº­p cÃ¡c giao diá»‡n

| Service | URL | Username | Password |
|---------|-----|----------|----------|
| Airflow | http://localhost:8080 | admin | admin |
| Spark UI | http://localhost:8081 | - | - |
| Grafana | http://localhost:3000 | admin | admin |
| Jupyter Lab | http://localhost:8888 | - | - |

### 2. Cháº¡y Pipeline

1. Truy cáº­p Airflow UI (http://localhost:8080)
2. ÄÄƒng nháº­p vá»›i admin/admin
3. TÃ¬m DAG `olist_data_pipeline`
4. Enable DAG vÃ  trigger cháº¡y

### 3. GiÃ¡m sÃ¡t Pipeline

- **Airflow**: Theo dÃµi task execution vÃ  logs
- **Spark UI**: GiÃ¡m sÃ¡t Spark jobs vÃ  performance
- **Grafana**: Dashboards cho business metrics
- **Jupyter**: Data exploration vÃ  ad-hoc analysis

## ğŸ“‹ CÃ¡c lá»‡nh há»¯u Ã­ch

```bash
# Khá»Ÿi Ä‘á»™ng pipeline
make start

# Dá»«ng pipeline
make stop

# Restart pipeline
make restart

# Cháº¡y dbt transformations
make dbt-run

# Cháº¡y dbt tests
make dbt-test

# Káº¿t ná»‘i PostgreSQL
make psql

# Má»Ÿ Spark shell
make spark-shell

# Backup database
make backup

# Dá»n dáº¹p
make clean

# Hiá»ƒn thá»‹ URLs
make urls
```

## ğŸ”„ Data Flow

```
CSV Files â†’ Raw Layer (Bronze) â†’ Staging Layer (Silver) â†’ Marts Layer (Gold) â†’ Analytics
    â†“              â†“                    â†“                   â†“              â†“
 Ingestion    Data Quality      Transformation        Aggregation    Visualization
```

### Luá»“ng xá»­ lÃ½ chi tiáº¿t:

1. **Data Ingestion**: Spark Ä‘á»c CSV files vÃ  load vÃ o raw schema
2. **Data Quality**: Great Expectations kiá»ƒm tra cháº¥t lÆ°á»£ng dá»¯ liá»‡u
3. **Data Transformation**: dbt transform data tá»« raw â†’ staging â†’ marts
4. **Data Validation**: Kiá»ƒm tra business rules vÃ  data consistency
5. **Analytics**: Táº¡o dashboards vÃ  reports trong Grafana

## ğŸ“Š Analytics & Reports

### Customer Analytics
- Customer segmentation (VIP, Premium, Regular, New)
- Customer lifetime value
- Purchase behavior analysis
- Geographic distribution

### Product Analytics  
- Best-selling products vÃ  categories
- Product performance metrics
- Inventory insights
- Pricing analysis

### Business Metrics
- Daily/Monthly sales trends
- Revenue analytics
- Order fulfillment metrics
- Customer satisfaction scores

## ğŸ§ª Data Quality

Pipeline tÃ­ch há»£p nhiá»u layers kiá»ƒm tra cháº¥t lÆ°á»£ng dá»¯ liá»‡u:

1. **Schema Validation**: Kiá»ƒm tra cáº¥u trÃºc dá»¯ liá»‡u
2. **Data Profiling**: PhÃ¢n tÃ­ch thá»‘ng kÃª dá»¯ liá»‡u
3. **Business Rules**: Kiá»ƒm tra logic nghiá»‡p vá»¥
4. **Completeness**: Kiá»ƒm tra tÃ­nh Ä‘áº§y Ä‘á»§
5. **Accuracy**: Kiá»ƒm tra tÃ­nh chÃ­nh xÃ¡c

## ğŸ”§ Cáº¥u hÃ¬nh

### Environment Variables

Táº¡o file `.env` trong thÆ° má»¥c `data_pipeline`:

```env
# Database
POSTGRES_DB=olist_dw
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres

# Airflow
AIRFLOW__CORE__FERNET_KEY=your-fernet-key
AIRFLOW__WEBSERVER__SECRET_KEY=your-secret-key

# Spark
SPARK_WORKER_MEMORY=2G
SPARK_WORKER_CORES=2
```

### Scaling

Äá»ƒ scale pipeline cho production:

1. **TÄƒng Spark workers**:
```yaml
spark-worker:
  deploy:
    replicas: 3
```

2. **Cáº¥u hÃ¬nh Airflow Executor**:
```yaml
AIRFLOW__CORE__EXECUTOR: CeleryExecutor
```

3. **ThÃªm monitoring**:
- Prometheus metrics
- ELK stack for logging
- Alerting vá»›i PagerDuty/Slack

## ğŸ“š TÃ i liá»‡u tham kháº£o

- [Apache Airflow Documentation](https://airflow.apache.org/docs/)
- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)
- [dbt Documentation](https://docs.getdbt.com/)
- [Great Expectations Documentation](https://docs.greatexpectations.io/)
- [Grafana Documentation](https://grafana.com/docs/)

## ğŸ”§ Development & Deployment

### Local Development

1. **Clone repository**:
```bash
git clone https://github.com/your-username/olist-data-pipeline.git
cd olist-data-pipeline
```

2. **Copy environment files**:
```bash
cp .env.example .env
cp dbt_project/profiles.yml.example dbt_project/profiles.yml
```

3. **Run setup**:
```bash
make setup
make up
```

### Production Deployment

- Thay Ä‘á»•i passwords trong `.env`
- Cáº¥u hÃ¬nh external databases
- Setup backup vÃ  monitoring
- Implement CI/CD pipeline

## ğŸš€ Roadmap

- [ ] ThÃªm CI/CD vá»›i GitHub Actions
- [ ] Implement streaming vá»›i Kafka
- [ ] ThÃªm ML models vá»›i MLflow
- [ ] Dashboard tá»± Ä‘á»™ng vá»›i Streamlit
- [ ] Data lineage vá»›i Apache Atlas
- [ ] Security scanning vá»›i Snyk

## ğŸ¤ Contributing

1. Fork repository
2. Táº¡o feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Táº¡o Pull Request

## ğŸ“„ License

Distributed under the MIT License. See `LICENSE` for more information.

## ğŸ“ Contact

- **Author**: Your Name
- **Email**: your.email@example.com
- **Project Link**: [https://github.com/your-username/olist-data-pipeline](https://github.com/your-username/olist-data-pipeline)

## ğŸ™ Acknowledgments

- [Olist Dataset](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce) on Kaggle
- Apache Software Foundation for amazing open-source tools
- dbt Labs for modern data transformation approach
- Docker community for containerization best practices

## ğŸ“„ License

MIT License - xem file [LICENSE](LICENSE) Ä‘á»ƒ biáº¿t thÃªm chi tiáº¿t.

---

**Happy Data Engineering!** ğŸ‰
